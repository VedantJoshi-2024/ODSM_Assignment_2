## Question: 

### (a) Main Difference Between L1 and L2 Regularization (Optimization Perspective)

1. **Objective Function Formulation**:  
   - **L2 (Ridge)**: Adds a penalty term proportional to the squared magnitude of coefficients:  

   $`
   \min_{\mathbf{w}} \|\mathbf{y}-\mathbf{X}\mathbf{w}\|_{2}^{2} + \alpha\|\mathbf{w}\|_{2}^{2}
   `$

   This results in a smooth, differentiable penalty that shrinks coefficients uniformly.  

   - **L1 (Lasso)**: Adds a penalty term proportional to the absolute magnitude of coefficients:  

   $`
   \min_{\mathbf{w}} \|\mathbf{y}-\mathbf{X}\mathbf{w}\|_{2}^{2} + \alpha\|\mathbf{w}\|_{1}
   `$

   The ℓ₁ penalty is non-differentiable at zero, leading to solutions where some coefficients are exactly zero.  

2. **Geometric Constraints**:  
   - **L2** constrains coefficients to lie within an ℓ₂-ball (sphere), promoting smooth shrinkage.  
   - **L1** constrains coefficients to lie within an ℓ₁-ball (diamond/octahedron), whose corners align with coordinate axes.  

3. **Solution Behavior**:  
   - **L2**: Yields dense solutions with all coefficients non-zero but shrunk toward zero.  
   - **L1**: Encourages sparse solutions by allowing coefficients to hit the "corners" of the ℓ₁-ball, setting some to zero.  

---

### (b) Why L1 Leads to Sparse Solutions (vs. L2)

1. **Geometric Intuition**:  
   - The ℓ₁-ball's corners lie on the coordinate axes (e.g., in 2D: $w_1 = 0$ or $w_2 = 0$). Minimizing the objective function often results in the solution intersecting these corners, forcing some coefficients to zero.  
   - The ℓ₂-ball's spherical shape has no such corners, so solutions rarely lie exactly on axes.  

2. **Mathematical Properties**:  
   - **L1's Non-Differentiability**: The absolute value in ℓ₁ regularization is non-differentiable at zero. Optimization algorithms (e.g., coordinate descent) explicitly zero out coefficients below a threshold (soft-thresholding operator):  

   $`
   \mathcal{S}_{\alpha}(z) = \text{sign}(z) \cdot \max \{|z|-\alpha, 0\}
   `$

   - **L2's Differentiability**: The ℓ₂ penalty is smooth everywhere, leading to proportional shrinkage without exact zeros.  

3. **Sparsity Mechanism**:  
   - **L1**: Small coefficients are driven to zero due to the "kink" in the ℓ₁ penalty at zero.  
   - **L2**: Coefficients are scaled by a factor of $1/(1+\alpha)$ but never reach zero unless $\alpha \to \infty$.  

4. **Practical Example**:  
   - For correlated features, L1 might select only one feature (zeroing others), while L2 retains all with reduced magnitudes.  

---

### Summary  
- **L1** exploits geometric and mathematical properties to enforce sparsity, making it ideal for feature selection.  
- **L2** provides smooth shrinkage, stabilizing solutions in ill-posed problems but retaining all features.
