## Question: High-Level Algorithm Descriptions

### (a) In your own words, how does PSO navigate the parameter space?

&emsp; In Particle Swarm Optimization (PSO), several particles are initialized with random velocities and positions in the parameter space as part of the algorithm's initialization process.

&emsp; Each particle keeps track of two key values:  
1. Its **personal best solution** (the best solution it has found so far).  
2. The **global best solution** (the best solution found by any particle in the swarm).  

&emsp; The particles update their velocities based on three components:  
1. **Momentum**: The particle maintains a portion of its initial velocity to continue moving in its current direction.  
2. **Cognitive intelligence**: The particle is attracted toward its personal best position.  
3. **Social intelligence**: The particle is attracted toward the global best position found by the swarm.  

The velocity update rule is given by:<br>
`V(t+1) = (w * V(t)) + (c1 * r1 * (P(t) – X(t))) + (c2 * r2 * (G(t) – X(t)))`

Where:  
- `V(t)` is the current velocity,  
- `w` is the inertia weight controlling momentum,  
- `c1` and `c2` are acceleration coefficients for cognitive and social components,  
- `r1` and `r2` are random numbers between 0 and 1,  
- `P(t)` is the personal best position,  
- `G(t)` is the global best position,  
- `X(t)` is the current position.  

&emsp; This equation balances **exploitation** (refining known good solutions) through momentum and **exploration** (searching new areas of the parameter space) through cognitive and social components. By combining these factors, PSO effectively navigates the parameter space to find optimal or near-optimal solutions.

---

### (b) How does GA generate new solutions, and why does it help maintain diversity?

&emsp; Genetic Algorithms (GA) generate new solutions inspired by biological evolution, specifically through processes such as **crossover** and **mutation**.

1. **Crossover**:  
   - A new solution is created by combining parts of two parent solutions.  
   - This process mimics genetic recombination in biology, where offspring inherit traits from both parents.  
   - For example, segments of one parent’s solution may be combined with segments from another parent to generate a new candidate solution.

2. **Mutation**:  
   - A new solution is generated by introducing random changes to an existing solution.  
   - This could involve adding random noise, flipping certain bits, or altering a small portion of the solution randomly.  
   - Mutation ensures that the algorithm explores areas of the search space that may not be reachable through crossover alone.

&emsp; By combining crossover and mutation, GA ensures that new solutions differ from their parent solutions. Crossover promotes exploration by recombining existing knowledge, while mutation introduces randomness to prevent premature convergence and explore unvisited areas of the search space.

&emsp; These mechanisms help maintain diversity in the population, which is crucial for avoiding local optima and ensuring a robust search across the entire parameter space.

---

### (c) How does SA decide whether to accept worse solutions, and how does that help in avoiding local minima?

&emsp; Simulated Annealing (SA) decides whether to accept worse solutions using a probabilistic approach inspired by the annealing process in metallurgy. This decision is governed by the **Metropolis criterion**, which is based on the difference in objective function values and a temperature parameter.

The probability of accepting a worse solution is given by:<br>
P = exp(-ΔE / T)


Where:  
- `ΔE` is the change in the objective function value (`new_solution_cost - current_solution_cost`),  
- `T` is the current temperature,  
- `exp` is the exponential function.  

&emsp; If `ΔE` is negative (i.e., the new solution is better), the solution is always accepted. However, if `ΔE` is positive (i.e., the new solution is worse), it may still be accepted with a probability proportional to `exp(-ΔE / T)`. As the temperature (`T`) decreases during the annealing process, the likelihood of accepting worse solutions diminishes.

&emsp; By occasionally accepting worse solutions, SA can escape local minima and explore other regions of the search space. This stochastic behavior allows the algorithm to "jump out" of suboptimal solutions and continue searching for a global optimum.  

&emsp; Early in the process, when the temperature is high, SA accepts worse solutions more frequently, promoting exploration of the parameter space. As the temperature decreases, the algorithm becomes more selective, focusing on exploitation to refine promising solutions.

&emsp; This balance between exploration (accepting worse solutions) and exploitation (focusing on better solutions) enables SA to avoid getting trapped in local minima and increases its chances of finding a global optimum.
